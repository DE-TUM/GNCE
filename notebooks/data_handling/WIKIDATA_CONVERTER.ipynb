{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a885e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rdflib.plugins.sparql.parser import parseQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90ed31",
   "metadata": {},
   "source": [
    "### Saving Wikidata5m.txt as proper turtle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cceab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open('../Datasets/wikidata/wikidata5m.txt','r'))\n",
    "\n",
    "with open('../Datasets/wikidata/wikidata5m.txt') as input_file:\n",
    "    entity_to_id = {}\n",
    "    entity_id_counter = 1\n",
    "    with open(\"wikidata.ttl\", \"w\") as output_file, \\\n",
    "         open(\"mapping_wikidata.json\", \"w\") as mapping_file:\n",
    "        for line in tqdm(input_file, total=num_lines):\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            \n",
    "            # Adding new entities to id set\n",
    "            if parts[0] not in entity_to_id:\n",
    "                entity_to_id[parts[0]] = entity_id_counter\n",
    "                entity_id_counter += 1\n",
    "            # Adding new entities to id set\n",
    "            if parts[1] not in entity_to_id:\n",
    "                entity_to_id[parts[1]] = entity_id_counter\n",
    "                entity_id_counter += 1\n",
    "            # Adding new entities to id set\n",
    "            if parts[2] not in entity_to_id:\n",
    "                entity_to_id[parts[2]] = entity_id_counter\n",
    "                entity_id_counter += 1\n",
    "                \n",
    "            s = \"<http://www.wikidata.org/entity/\" + str(entity_to_id[parts[0]]) + \">\"\n",
    "            p = \"<http://www.wikidata.org/prop/direct/\" + str(entity_to_id[parts[1]]) + \">\"\n",
    "            o = \"<http://www.wikidata.org/entity/\" + str(entity_to_id[parts[2]]) + \">\"\n",
    "            \n",
    "            s = \"<http://www.wikidata.org/entity/\" + parts[0] + \">\"\n",
    "            p = \"<http://www.wikidata.org/prop/direct/\" + parts[1] + \">\"\n",
    "            o = \"<http://www.wikidata.org/entity/\" + parts[2] + \">\"\n",
    "\n",
    "            output_file.write(s + \" \" + p + \" \" + o + \" .\")\n",
    "            output_file.write(\"\\n\")\n",
    "            #output_file.write(f\"{entity_to_id[parts[0]]}\\t{entity_to_id[parts[1]]}\\t{entity_to_id[parts[2]]}\\n\")\n",
    "            \n",
    "          # Write the mapping dictionaries to the mapping file as JSON\n",
    "        json.dump({\"description\": \"Mapping of original entity names to unique ids. Entities and Predicates share the id set, i.e. there does not exist the same id for an entity and a predicate\", \"entity_to_id\": entity_to_id}, mapping_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60930c6a",
   "metadata": {},
   "source": [
    "## Wikidata Turtle ID Mapping for LMKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mapping = {}\n",
    "predicate_mapping = {}\n",
    "entity_counter = 0\n",
    "predicate_counter = 0\n",
    "\n",
    "\n",
    "num_lines = sum(1 for line in open('../Datasets/wikidata/graph/wikidata.nt','r'))\n",
    "\n",
    "with open(\"../Datasets/wikidata/graph/wikidata.nt\", \"r\") as f:\n",
    "    for line in tqdm(f, total=num_lines):\n",
    "        triple = line.split(\" \")[:-1]\n",
    "        if not triple[0] in entity_mapping:\n",
    "            entity_mapping[triple[0]] = entity_counter\n",
    "            entity_counter += 1\n",
    "        if not triple[2] in entity_mapping:\n",
    "            entity_mapping[triple[2]] = entity_counter\n",
    "            entity_counter += 1\n",
    "        if not triple[1] in predicate_mapping:\n",
    "            predicate_mapping[triple[1]] = predicate_counter\n",
    "            predicate_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datasets/wikidata/entity_mapping.json', 'w') as json_file:\n",
    "    json.dump(entity_mapping, json_file)\n",
    "with open('../Datasets/wikidata/predicate_mapping.json', 'w') as json_file:\n",
    "    json.dump(predicate_mapping, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446e6a8",
   "metadata": {},
   "source": [
    "### Transforming and executing real queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac703b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries = pd.read_csv('../Datasets/wikidata/I1_status2xx_userData_Joined.tsv', sep='\\t')\n",
    "queries2 = pd.read_csv('/home/tim/Downloads/2018-02-26_2018-03-25_all.tsv.gz', sep='\\t', nrows=10000000,\n",
    "                      skiprows=range(1,10000000+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c655ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.concat([queries, queries2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7d3ce",
   "metadata": {},
   "source": [
    "# Transforming Queries to GNCE Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f00e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_list = ['FILTER', \"UNION\", \"OPTIONAL\", \"SERVICE\", \"COUNT\", '\"', \"BIND\", \"DISTINCT\", \"GROUP\", \"LIMIT\",\n",
    "              \"VALUES\", \"ASKWHERE\", \"ASK\"]\n",
    "endpoint_url = \"http://localhost:8907/sparql\"\n",
    "sparql = SPARQLWrapper(endpoint_url)\n",
    "\n",
    "new_queries = []\n",
    "for q in tqdm(queries[\"anonymizedQuery\"]):\n",
    "    q = urllib.parse.unquote_plus(q).replace(\"\\n\", \"\")\n",
    "    \n",
    "    if any(s in q for s in string_list):\n",
    "        continue\n",
    "    skip = False\n",
    "    \n",
    "    q = {\"query\": q}\n",
    "    \n",
    "    if 'Describe' in q['query'] or 'VALUES' in q['query'] or 'MINUS' in q['query']:\n",
    "        continue\n",
    "    try:\n",
    "        triples = [t.replace(\" * \", \"  \").split('  ') for t in q['query'].split('{')[1].split('}')[0].split(\" . \")]\n",
    "    except:\n",
    "        print(q)\n",
    "    #for q in queries:\n",
    "    for t in  triples:\n",
    "        if \"\" in t:\n",
    "            t.remove(\"\")\n",
    "        if \" * \" in t:\n",
    "            t.replace(\" * \", \"  \")\n",
    "        for i in range(len(t)):\n",
    "            t[i] = t[i].replace(\" .\", \"\")\n",
    "            t[i] = t[i].replace(\" \", \"\")\n",
    "        q['triples'] = triples\n",
    "    entities = []\n",
    "    for t in triples:\n",
    "        if len(t) < 3:\n",
    "            skip =True\n",
    "            break\n",
    "        for e in t:\n",
    "            if \";\" in e:\n",
    "                skip=True\n",
    "                break\n",
    "\n",
    "        if not \"?\" in t[0]:\n",
    "            entities.append(t[0].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[2]:\n",
    "            entities.append(t[2].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[1]:\n",
    "            entities.append(t[1].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "    q[\"x\"] = entities\n",
    "    # Evaluating the Query over the Endpoint\n",
    "\n",
    "    if not skip:\n",
    "        try:\n",
    "            sparql.setQuery(q['query'])\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            results = sparql.query().convert()\n",
    "            #print(results)\n",
    "        except urllib.error.URLError:\n",
    "            input(\"Restart Virtuoso\")\n",
    "            #raise\n",
    "            continue\n",
    "        except:\n",
    "            continue\n",
    "        if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "            if i%10 == 0:\n",
    "                print(len(cleaned_queries))\n",
    "                i += 1\n",
    "            q['y'] = len(results[\"results\"][\"bindings\"])\n",
    "            new_queries.append(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Wikidata_user_queries8.json\", \"w\") as file:\n",
    "    json.dump(new_queries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674e03d",
   "metadata": {},
   "source": [
    "### Final Cleaning to remove entities not occurring in Graph File: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/tim/Datasets/wikidata/user/Joined_Queries.json\", \"r\") as file:\n",
    "    queries = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca79a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/tim/Datasets/wikidata/entity_mapping.json\", \"r\") as f:\n",
    "    entity_mapping = json.load(f)\n",
    "with open(\"/home/tim/Datasets/wikidata/predicate_mapping.json\", \"r\") as f:\n",
    "    predicate_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of entity and predicate ids to separate ids:\n",
    "with open(\"/home/tim/Datasets/wikidata/id_to_id_mapping.json\", \"r\") as f:\n",
    "    id_to_id_mapping = json.load(f)\n",
    "    \n",
    "with open(\"/home/tim/Datasets/wikidata/id_to_id_mapping_predicate.json\", \"r\") as f:\n",
    "    id_to_id_mapping_predicate = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a633cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Laoding Prone labels\n",
    "with open(\"/home/tim/LSS/wikidata_mapping.txt\", \"r\") as file:\n",
    "    lines = file.readlines()  # Read all lines of the file and store them in a list\n",
    "\n",
    "# Strip the newline character '\\n' from each line and store the cleaned lines in a new list\n",
    "cleaned_lines = [line.strip() for line in lines]\n",
    "\n",
    "new_queries = []\n",
    "for q in queries:\n",
    "    add = True\n",
    "    for tp in q['triples']:\n",
    "        if entity_mapping:\n",
    "            if not \"?\" in tp[0]:\n",
    "                if not tp[0] in entity_mapping:\n",
    "                    print(\"Skipping\")\n",
    "                    add = False\n",
    "                    break\n",
    "            if not \"?\" in tp[2]:\n",
    "                if not tp[2] in entity_mapping:\n",
    "                    add = False\n",
    "                    print(\"Skipping\")\n",
    "                    break\n",
    "        if predicate_mapping:\n",
    "            if not \"?\" in tp[1]:\n",
    "                if not tp[1] in predicate_mapping:\n",
    "                    add = False\n",
    "                    break\n",
    "    # Cleaning away queries that have a triple with class assignment that is not in graph:\n",
    "        if tp[1] == '<http://www.wikidata.org/prop/direct/P31>':\n",
    "            if not \"?\" in tp[2]:\n",
    "                if not str(id_to_id_mapping[tp[2]]) in cleaned_lines:\n",
    "                    print(tp[2])\n",
    "                    print(str(id_to_id_mapping[tp[2]]))\n",
    "                    print(\"DontKnow that class\")\n",
    "                    add = False\n",
    "                    break\n",
    "    if add:\n",
    "        new_queries.append(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc625c7",
   "metadata": {},
   "source": [
    "## Cleaning Queries with Unknown Class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80349d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29656108",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/tim/Datasets/wikidata/user/Joined_Queries.json\", \"w\") as file:\n",
    "    json.dump(new_queries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be087c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = []\n",
    "for q in queries:\n",
    "    skip = False\n",
    "    if 'Describe' in q['query'] or 'VALUES' in q['query'] or 'MINUS' in q['query']:\n",
    "        continue\n",
    "    try:\n",
    "        triples = [t.replace(\" * \", \"  \").split('  ') for t in q['query'].split('{')[1].split('}')[0].split(\" . \")]\n",
    "    except:\n",
    "        print(q)\n",
    "    #for q in queries:\n",
    "    for t in  triples:\n",
    "        if \"\" in t:\n",
    "            t.remove(\"\")\n",
    "        if \" * \" in t:\n",
    "            t.replace(\" * \", \"  \")\n",
    "        for i in range(len(t)):\n",
    "            t[i] = t[i].replace(\" .\", \"\")\n",
    "            t[i] = t[i].replace(\" \", \"\")\n",
    "        q['triples'] = triples\n",
    "    entities = []\n",
    "    for t in triples:\n",
    "        if len(t) < 3:\n",
    "            skip =True\n",
    "            break\n",
    "        for e in t:\n",
    "            if \";\" in e:\n",
    "                skip=True\n",
    "                break\n",
    "\n",
    "        if not \"?\" in t[0]:\n",
    "            entities.append(t[0].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[2]:\n",
    "            entities.append(t[2].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[1]:\n",
    "            entities.append(t[1].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "    q[\"x\"] = entities\n",
    "    if not skip:\n",
    "        new_queries.append(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Cleaned_wikidata_User_queries2.json', 'r') as file:\n",
    "    # Load the JSON data\n",
    "    queries = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4325cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = []\n",
    "for q in queries:\n",
    "    skip = False\n",
    "    if 'Describe' in q['query'] or 'VALUES' in q['query'] or 'MINUS' in q['query']:\n",
    "        continue\n",
    "    try:\n",
    "        triples = [t.replace(\" * \", \"  \").split('  ') for t in q['query'].split('{')[1].split('}')[0].split(\" . \")]\n",
    "    except:\n",
    "        print(q)\n",
    "    #for q in queries:\n",
    "    for t in  triples:\n",
    "        if \"\" in t:\n",
    "            t.remove(\"\")\n",
    "        if \" * \" in t:\n",
    "            t.replace(\" * \", \"  \")\n",
    "        for i in range(len(t)):\n",
    "            t[i] = t[i].replace(\" .\", \"\")\n",
    "            t[i] = t[i].replace(\" \", \"\")\n",
    "        q['triples'] = triples\n",
    "    entities = []\n",
    "    for t in triples:\n",
    "        if len(t) < 3:\n",
    "            skip =True\n",
    "            break\n",
    "        for e in t:\n",
    "            if \";\" in e:\n",
    "                skip=True\n",
    "                break\n",
    "\n",
    "        if not \"?\" in t[0]:\n",
    "            entities.append(t[0].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[2]:\n",
    "            entities.append(t[2].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "        if not \"?\" in t[1]:\n",
    "            entities.append(t[1].replace(\"<\",\"\").replace(\">\",\"\").replace(\" .\", \"\"))\n",
    "    q[\"x\"] = entities\n",
    "    if not skip:\n",
    "        new_queries.append(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in new_queries:\n",
    "    for e in q[\"x\"]:\n",
    "        if \";\" in e:\n",
    "            print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a980f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Wikidata_user_queries.json\", \"w\") as file:\n",
    "    json.dump(new_queries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000000000\n",
    "plt.hist([c for c in cardinalities if c < limit and c > 1])\n",
    "plt.yscale(\"log\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
