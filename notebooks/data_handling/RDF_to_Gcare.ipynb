{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c01623",
   "metadata": {},
   "source": [
    "# This Script transforms a .ttl file with URIS containing ids to a .txt file in the \"G-Care\" format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of entities and predicates\n",
    "vertices = set()\n",
    "predicates = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ebaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For which dataset to load the turtle or nt file\n",
    "dataset = \"yago_inductive\"\n",
    "# Under which name to save the resulting g-care graph\n",
    "gcare_graph_savename = \"yago_inductive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ada4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare which URI corresponds to rdf:type\n",
    "if dataset=='swdf':\n",
    "    rdf_type_uri = '<http://ex.org/03>'\n",
    "elif dataset=='yago' or dataset=='yago_inductive':\n",
    "    rdf_type_uri = \"<http://example.com/13000179>\"\n",
    "elif dataset =='wikidata':\n",
    "    rdf_type_uri = \"<http://www.wikidata.org/prop/direct/P31>\"\n",
    "elif dataset =='lubm':\n",
    "    rdf_type_uri = \"<http://example.org/1>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: define a set of entities to exclude from the graph. Those wont be stored as vertices,\n",
    "# and related edges are also not stored\n",
    "excluded_entities = set()\n",
    "\n",
    "excluded_query_type = 'star'\n",
    "# In this case, we open the inductive test set from the graph\n",
    "# Specifically, we are adding all the objects only, as the subjects are always variables,\n",
    "# and the predicates are assumed to be known\n",
    "\n",
    "\n",
    "with open(f\"/home/tim/Datasets/{dataset}/{excluded_query_type}/disjoint_test.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "for query in test_data:\n",
    "    excluded_entities.update([a[2] for a in query[\"triples\"] if not \"?\" in a[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(excluded_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb16958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all entities and predicates from the .ttl file\n",
    "n_excluded_entites = set()\n",
    "l = 0\n",
    "ttl_file = open(f\"/home/tim/Datasets/{dataset}/graph/{dataset}.nt\", \"r\")\n",
    "for line in tqdm(ttl_file):\n",
    "    atoms = line.split(\" \")[:-1]\n",
    "    #if not atoms == []:\n",
    "    if True:\n",
    "        l += 1\n",
    "        if not atoms[2] in excluded_entities:\n",
    "            vertices.add(atoms[0])\n",
    "        if not atoms[2] in excluded_entities:\n",
    "            vertices.add(atoms[2])\n",
    "        else:\n",
    "            n_excluded_entites.update([atoms[2]])\n",
    "        predicates.add(atoms[1])\n",
    "        \n",
    "print('Finished collecting vertices and predicates')\n",
    "print(f'Excluded {len(n_excluded_entites)} entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(excluded_entities)==len(n_excluded_entites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb38a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to map the URL ids to entity and predicate ids\n",
    "id_to_id_mapping = {}\n",
    "id_to_id_mapping_predicate = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vertex Dict and save entity mappings\n",
    "vertex_dict = {}\n",
    "vid = 0\n",
    "for vertex in tqdm(vertices):\n",
    "    dvid = vertex.split(\"/\")[-1].replace(\">\", \"\")\n",
    "    #vertex_dict[vertex] = [dvid]\n",
    "    vertex_dict[vertex] = [vid]\n",
    "    id_to_id_mapping[vertex] = vid\n",
    "    vid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be799a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Mappings for Predicates\n",
    "pid = 0\n",
    "for p in predicates:\n",
    "    id_to_id_mapping_predicate[p] = pid\n",
    "    pid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94632e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Vertex types\n",
    "ttl_file = open(f\"/home/tim/Datasets/{dataset}/graph/{dataset}.nt\", \"r\")\n",
    "for line in tqdm(ttl_file):\n",
    "    atoms = line.split(\" \")[:-1]\n",
    "    if not (atoms[0] in excluded_entities) and not (atoms[2] in excluded_entities):\n",
    "        # If triple has predicate rdf:type\n",
    "        if atoms[1] == rdf_type_uri:\n",
    "            vertex_dict[atoms[0]] += vertex_dict[atoms[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Default Label if entity has no types:\n",
    "for v in vertex_dict:\n",
    "    if len(vertex_dict[v]) == 1:\n",
    "        vertex_dict[v].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Edge List\n",
    "n_skipped_edges = 0\n",
    "edge_list = []\n",
    "ttl_file = open(f\"/home/tim/Datasets/{dataset}/graph/{dataset}.nt\", \"r\")\n",
    "for tp in tqdm(ttl_file):\n",
    "    tp = tp.split(\" \")[:-1]\n",
    "    if not (tp[0] in excluded_entities) and not (tp[2] in excluded_entities):\n",
    "    #edge_label = tp[1].split(\"/\")[-1].replace(\">\", \"\") if not \"?\" in tp[1] else -1\n",
    "        edge_list.append([vertex_dict[tp[0]][0], vertex_dict[tp[2]][0], id_to_id_mapping_predicate[tp[1]]])\n",
    "    else:\n",
    "        n_skipped_edges +=1\n",
    "print('Finished creating edge list')\n",
    "print(f'Dropped a total of {n_skipped_edges} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the Data File\n",
    "with open(f\"/home/tim/gcare/data/dataset/{dataset}/{gcare_graph_savename}.txt\", \"w\") as f:\n",
    "    f.write(\"t # 1\")\n",
    "    f.write(\"\\n\")\n",
    "    for v in tqdm(vertex_dict):\n",
    "        f.write(\"v\")\n",
    "        for p in vertex_dict[v]:\n",
    "            f.write(\" \")\n",
    "            f.write(str(p) + \"\")\n",
    "        #f.write(\"v \" + str(vertex_dict[v][0]) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "    for e in tqdm(edge_list):\n",
    "        f.write(\"e \" + str(e[0]) + \" \" + str(e[1]) + \" \" + str(e[2]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to ID to ID mapping for later query transformation\n",
    "with open(f\"/home/tim/Datasets/{dataset}/id_to_id_mapping_{gcare_graph_savename}.json\", \"w\") as f:\n",
    "    json.dump(id_to_id_mapping, f)\n",
    "\n",
    "with open(f\"/home/tim/Datasets/{dataset}/id_to_id_mapping_predicate_{gcare_graph_savename}.json\", \"w\") as f:\n",
    "    json.dump(id_to_id_mapping_predicate, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfc080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
